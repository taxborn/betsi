# Lite Observation
[arXiv paper](https://arxiv.org/abs/1706.03762)
A light implementation of the 2017 Google paper 'Attention is all you need'.

For this implementation I will implement a translation from English to Spanish, as Tranformer models are exceptional at language 
translation and this seems to be a common use of light implementations of this paper.

The dataset I will be using is the [opus books](https://opus.nlpl.eu/Books.php) dataset which is a collection of copyright free books.
The book content of these translations are free for personal, educational, and research use. 
[OPUS language resource paper](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf).

## TODO:
- [ ] Input Embeddings
- [ ] Positional Encoding
- [ ] Layer Normalization
- [ ] Feed forward
- [ ] Multi-Head attention
- [ ] Residual Connection
- [ ] Encoder
- [ ] Decoder
- [ ] Linear Layer
- [ ] Transformer
- [ ] Tokenizer
- [ ] Dataset ()
- [ ] Training loop
- [ ] Visualization of the model

## 
